{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenny_jung/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:1.13.1\n",
      "Cuda version: None\n",
      "transformers version: 4.39.1\n",
      "GPU 사용 가능여부: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n",
    "print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n",
    "print(\"transformers version: {}\".format(transformers.__version__)) # transformers 4.28.0\n",
    "print(\"GPU 사용 가능여부: {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:2.2.1\n",
      "MPS 장치를 지원하도록 build 되었는지: True\n",
      "MPS 장치가 사용 가능한지: True\n"
     ]
    }
   ],
   "source": [
    "print (f\"PyTorch version:{torch.__version__}\") # 1.12.1 이상\n",
    "print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\") # True 여야 합니다.\n",
    "print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\") # True 여야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "tensor([-0.5661, -0.0711,  0.1146,  0.1991, -0.7503], device='mps:0',\n",
      "       grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    " \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(5,5)\n",
    " \n",
    "    def forward(self,x):\n",
    "        net = self.lin1(x)\n",
    "        return net\n",
    " \n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")\n",
    " \n",
    "# MPS 장치에 바로 tensor를 생성합니다.\n",
    "x = torch.ones(5, device=device)\n",
    " \n",
    "# GPU 상에서 연산 진행\n",
    "y = x * 2\n",
    " \n",
    "# 또는, 다른 장치와 마찬가지로 MPS로 이동할 수도 있습니다.\n",
    "model = Net()# 어떤 모델의 객체를 생성한 뒤,\n",
    "model.to(device) # MPS 장치로 이동합니다.\n",
    " \n",
    "# 이제 모델과 텐서를 호출하면 GPU에서 연산이 이뤄집니다.\n",
    "pred = model(x)\n",
    "print(pred)\n",
    "\n",
    "# MPS 장치를 사용하는 경우, torch.cuda.is_available()은 False를 반환합니다.\n",
    "# 예측 결과에 device='mps:0'로 나오면 간단한 모델을 통한 예측에서 mps를 정상적으로 사용한 것입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Base model and Dataset for RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1 Importing tokenizer and model using Huggingface's transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2 Maximum tokens by the imported tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai-community/gpt2': 1024,\n",
       " 'openai-community/gpt2-medium': 1024,\n",
       " 'openai-community/gpt2-large': 1024,\n",
       " 'openai-community/gpt2-xl': 1024,\n",
       " 'distilbert/distilgpt2': 1024}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3 Testing tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"이제 봄이 오네요. 꽃들이 피어나기 시작했습니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(input_txt).tokens()\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kogpt-2_tokens</th>\n",
       "      <td>▁이제</td>\n",
       "      <td>▁봄이</td>\n",
       "      <td>▁오</td>\n",
       "      <td>네</td>\n",
       "      <td>요.</td>\n",
       "      <td>▁꽃</td>\n",
       "      <td>들이</td>\n",
       "      <td>▁피</td>\n",
       "      <td>어나</td>\n",
       "      <td>기</td>\n",
       "      <td>▁시작</td>\n",
       "      <td>했</td>\n",
       "      <td>습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input_IDs</th>\n",
       "      <td>12857</td>\n",
       "      <td>46339</td>\n",
       "      <td>9114</td>\n",
       "      <td>7098</td>\n",
       "      <td>25856</td>\n",
       "      <td>9718</td>\n",
       "      <td>9136</td>\n",
       "      <td>9301</td>\n",
       "      <td>9537</td>\n",
       "      <td>6958</td>\n",
       "      <td>9347</td>\n",
       "      <td>8718</td>\n",
       "      <td>16691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0      1     2     3      4     5     6     7     8     9   \\\n",
       "kogpt-2_tokens    ▁이제    ▁봄이    ▁오     네     요.    ▁꽃    들이    ▁피    어나     기   \n",
       "Input_IDs       12857  46339  9114  7098  25856  9718  9136  9301  9537  6958   \n",
       "\n",
       "                  10    11     12  \n",
       "kogpt-2_tokens   ▁시작     했   습니다.  \n",
       "Input_IDs       9347  8718  16691  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 40\n",
    "pd.options.display.max_rows = 60\n",
    "df = pd.DataFrame([tokens, input_ids[0]], index=[\"kogpt-2_tokens\", \"Input_IDs\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4 Testing decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이제 봄이 오네요. 꽃들이 피어나기 시작했습니다. 네. 꽃들이 피어나기 시작했습니다. 네. 꽃들이 피어나기 시작했습니다. 네. 꽃들이 피어나기 시작했습니다. 네. 꽃들이 피어나기 시작했습니다. 네. 꽃들이 피어나기 시작했습니다. 네. 꽃들이 피어나기 시작했습니다. 네. 꽃들이 피어나기 시작했습니다. 네. 꽃들이 피어나기 시작했습니다. 네. 꽃들이 피어나기 시작했습니다. 네. 꽃들이 피어나기 시작했습니다. 네. 꽃들이 피어나기 시작했습니다. 네. 꽃들이 피\n"
     ]
    }
   ],
   "source": [
    "max_length=128\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False) # greedy decoding\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이제 봄이 오네요. 꽃들이 피어나기 시작했습니다. 네. 봄이 왔습니다. 예. 꽃샘추위가 기승을 부리고 있는데요. 어서 오셔야겠습니다. 자 오늘 날씨 전해주시죠. 오늘은 전국이 대체로 맑은 날씨를 보이겠고요.\n",
      "낮기온은 서울이 영하 십오 도 등 어제와 비슷하거나 조금 높겠습니다.\n",
      "다음 주 월요일에는 전국에 또 한 차례 봄비가 내릴 것으로 전망됩니다.</d> 오늘 아침신문 살펴보는 시간입니다.\n",
      "먼저 중앙일보입니다.\n",
      "지난 주말 서울 도심에서 대규모 촛불집회가 열렸습니다.\n",
      "이번 집회에는 주최 측\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=10, no_repeat_ngram_size=2, # beam search decoding with n-gram penalty\n",
    "                             do_sample=False)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이제 봄이 오네요. 꽃들이 피어나기 시작했습니다. 그죠? 예. 봄꽃이 피기 시작한 겁니다. 네. 자 오늘 두 번째 키워드 보겠습니다. 자 첫 번째 키워드는 한여름 폭염입니다.\n",
      "올해 삼월에 폭염이 시작됐습니다.\n",
      "폭염이 예년보다 훨씬 더 오래 지속될 것으로 예상이 돼서 그렇죠. 지난 이월 십오일부터 이십삼일까지 일 점 오 퍼센트 이상 올랐다고 하는데요.\n",
      "그런데 이 폭염은 올해로 일년째 이어지고 있는데요.\n",
      "네. 올해는 이백육십팔일에서 삼십육일 사이 더 높다고 합니다.\n",
      "예\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, temperature=2.0, top_k=50) # beam search decoding with n-gram penalty and sampling\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이제 봄이 오네요. 꽃들이 피어나기 시작했습니다. 예. 네. 봄기운이 완연해지는 것 같습니다. 아~ 그렇군요. 그렇죠. 예예. 자~ 오늘은 여기까지 듣겠습니다. 고맙습니다. 감사합니다. 감사합니다.\n",
      "네. 지금까지 티비에스 앱 오십 원 유료 문자 샵 영구오일 카카오톡 플러스 친구 맺으시면 무료 메시지 보내실 수 있구요. 또 무료 쿠폰도 함께 보내주시기 바랍니다.</d> 오늘 아침신문 보겠습니다.\n",
      "먼저 조선일보입니다.\n",
      "이천십육년 십일월 이십구일 서울 광화문 광\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, top_p=0.90) # beam search decoding with n-gram penalty and nucleus sampling\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5 Loading SFT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '불고기용 고기 한우에요?',\n",
       "  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
       "  'tokens': 193},\n",
       " {'prompt': '쓰던 앱이 유료로 전환됐어',\n",
       "  'completion': \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\",\n",
       "  'tokens': 288},\n",
       " {'prompt': '여친이랑 다툼',\n",
       "  'completion': \"'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지만 다투게 되면 상대방의 말을 경청하는 태도와 서로의 감정을 존중하는 것이 중요합니다. 논리적이고 차분한 대화를 통해 문제를 해결해보세요. 그리고 서로를 이해하고 서로의 의견을 수용하는 것이 중요합니다.\",\n",
       "  'tokens': 153}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "data_path_1_SFT = '/Users/kenny_jung/aiffel/data/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl' \n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-6 Loading RM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?',\n",
       "  'completion_0': 'Allow me to answer your question. I know that you are curious about me.',\n",
       "  'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.',\n",
       "  'completion_2': '라이언에게 말했다.',\n",
       "  'ranking': [2, 1, 0]},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?',\n",
       "  'completion_0': '개포주공아파트는 다섯 단지로 이루어져 있습니다.',\n",
       "  'completion_1': '이날 목송에서 구글상위노',\n",
       "  'completion_2': '개포주공아파트는 총 27개 단지로 이루어져 있습니다.',\n",
       "  'ranking': [2, 0, 1]},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?',\n",
       "  'completion_0': 'The diameter of the Metallic domain is bigger than the Hyperonic domain.',\n",
       "  'completion_1': '이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가 그 발언을 문제삼았는지에 따라 답이 다를 수 있습니다.\\\\n\\\\n만약 김영삼 대통령이 후보 시절에 지역표심을 겨냥한 발언을 했다는 가정하에, 그 발언을 문제삼은 후보가 누구였는지를 대답하자면, 그 답은 이화선 당시 민주당 대통령 후보가 될 것입니다. 1992년 총선 때, 김영삼 대선후보는 \"집값이 오른 노량진역 부근의 부동산 가격은 세월호 폭침 후 \\\\\\'강남 도시재생\\\\\\' 일환으로 상승했다\"는 발언을 했습니다. 하지만 이화선 후보는 이 발언을 \"전국적으로 경제적 발전이 이루어지지 않은 지방민의 마음을 멀리해지려는 무례한 발언\"이라고 비판하며 문제삼았습니다.\\\\n\\\\n하지만, 이 질문을 답변하는 데 있어서 보다 명확한 정보가 있으면 답변을 보완할 수 있습니다.',\n",
       "  'completion_2': '김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 추구하고 있는 민주주의 광범위하게 확립과 보수의 사상을 이어가는 데 있어 지역경제 발전과 공공서비스 신속 개선을 위해 합리적인 국가 정책에 따르는 방향성을 제시하고 있습니다.',\n",
       "  'ranking': [1, 2, 0]}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_2_RM = '/Users/kenny_jung/aiffel/data/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "with open(data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-7 Loading PPO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?'},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?'},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_3_PPO = '/Users/kenny_jung/aiffel/data/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 SFT kogpt-2 with instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 Import model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<usr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<sys>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"<d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"</d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\"<unused0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\"<unused1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<unused2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t12: AddedToken(\"<unused3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t13: AddedToken(\"<unused4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t14: AddedToken(\"<unused5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t15: AddedToken(\"<unused6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t16: AddedToken(\"<unused7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t17: AddedToken(\"<unused8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t18: AddedToken(\"<unused9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t19: AddedToken(\"<unused10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t20: AddedToken(\"<unused11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t21: AddedToken(\"<unused12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t22: AddedToken(\"<unused13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t23: AddedToken(\"<unused14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t24: AddedToken(\"<unused15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t25: AddedToken(\"<unused16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t26: AddedToken(\"<unused17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t27: AddedToken(\"<unused18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t28: AddedToken(\"<unused19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t29: AddedToken(\"<unused20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t30: AddedToken(\"<unused21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t31: AddedToken(\"<unused22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32: AddedToken(\"<unused23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t33: AddedToken(\"<unused24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t34: AddedToken(\"<unused25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t35: AddedToken(\"<unused26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t36: AddedToken(\"<unused27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t37: AddedToken(\"<unused28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t38: AddedToken(\"<unused29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t39: AddedToken(\"<unused30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t40: AddedToken(\"<unused31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t41: AddedToken(\"<unused32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t42: AddedToken(\"<unused33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t43: AddedToken(\"<unused34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t44: AddedToken(\"<unused35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t45: AddedToken(\"<unused36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t46: AddedToken(\"<unused37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t47: AddedToken(\"<unused38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t48: AddedToken(\"<unused39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t49: AddedToken(\"<unused40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50: AddedToken(\"<unused41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t51: AddedToken(\"<unused42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t52: AddedToken(\"<unused43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t53: AddedToken(\"<unused44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t54: AddedToken(\"<unused45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t55: AddedToken(\"<unused46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t56: AddedToken(\"<unused47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t57: AddedToken(\"<unused48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t58: AddedToken(\"<unused49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t59: AddedToken(\"<unused50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t60: AddedToken(\"<unused51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t61: AddedToken(\"<unused52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t62: AddedToken(\"<unused53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t63: AddedToken(\"<unused54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t64: AddedToken(\"<unused55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t65: AddedToken(\"<unused56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t66: AddedToken(\"<unused57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t67: AddedToken(\"<unused58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t68: AddedToken(\"<unused59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t69: AddedToken(\"<unused60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t70: AddedToken(\"<unused61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t71: AddedToken(\"<unused62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t72: AddedToken(\"<unused63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t73: AddedToken(\"<unused64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t74: AddedToken(\"<unused65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t75: AddedToken(\"<unused66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t76: AddedToken(\"<unused67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t77: AddedToken(\"<unused68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t78: AddedToken(\"<unused69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t79: AddedToken(\"<unused70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t80: AddedToken(\"<unused71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t81: AddedToken(\"<unused72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t82: AddedToken(\"<unused73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t83: AddedToken(\"<unused74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t84: AddedToken(\"<unused75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t85: AddedToken(\"<unused76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t86: AddedToken(\"<unused77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t87: AddedToken(\"<unused78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t88: AddedToken(\"<unused79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t89: AddedToken(\"<unused80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t90: AddedToken(\"<unused81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t91: AddedToken(\"<unused82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92: AddedToken(\"<unused83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t93: AddedToken(\"<unused84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t94: AddedToken(\"<unused85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t95: AddedToken(\"<unused86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t96: AddedToken(\"<unused87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t97: AddedToken(\"<unused88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t98: AddedToken(\"<unused89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t99: AddedToken(\"<unused90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"<unused91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"<unused92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"<unused93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"<unused94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t104: AddedToken(\"<unused95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t105: AddedToken(\"<unused96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t106: AddedToken(\"<unused97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t107: AddedToken(\"<unused98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t108: AddedToken(\"<unused99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t109: AddedToken(\":-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t110: AddedToken(\":)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t111: AddedToken(\"-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t112: AddedToken(\"(-:\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t113: AddedToken(\"(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t114: AddedToken(\"(:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t115: AddedToken(\"-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t116: AddedToken(\"8-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t117: AddedToken(\"'-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t118: AddedToken(\":-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t119: AddedToken(\":-*\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t120: AddedToken(\":-/\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t121: AddedToken(\":->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t122: AddedToken(\":-@\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t123: AddedToken(\":-d\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t124: AddedToken(\":-V\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t125: AddedToken(\":-X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t126: AddedToken(\":-\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t127: AddedToken(\":-]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128: AddedToken(\";-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t129: AddedToken(\">;->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t130: AddedToken(\";^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t131: AddedToken(\"%-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t132: AddedToken(\"):-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t133: AddedToken(\"3:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t134: AddedToken(\":-&\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t135: AddedToken(\"8:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t136: AddedToken(\":-)8<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t137: AddedToken(\":-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t138: AddedToken(\":-6\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t139: AddedToken(\"+:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t140: AddedToken(\"O:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t141: AddedToken(\":-<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t142: AddedToken(\":-?\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t143: AddedToken(\":-E\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t144: AddedToken(\":-Q\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t145: AddedToken(\":-}X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t146: AddedToken(\":-[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t147: AddedToken(\":-a\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t148: AddedToken(\":-{\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t149: AddedToken(\":-{}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t150: AddedToken(\":^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151: AddedToken(\"<:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t152: AddedToken(\":=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t153: AddedToken(\">:->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t154: AddedToken(\">:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t155: AddedToken(\"@:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t156: AddedToken(\"@:-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t157: AddedToken(\"C=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t158: AddedToken(\"X:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t159: AddedToken(\"[:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t160: AddedToken(\"[:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t161: AddedToken(\"{:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t162: AddedToken(\"l^o\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t163: AddedToken(\"}:^#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t164: AddedToken(\":-(=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t165: AddedToken(\"O-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t166: AddedToken(\":-3\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t167: AddedToken(\":=\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t168: AddedToken(\":-\"\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t169: AddedToken(\"P-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t170: AddedToken(\"?-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t171: AddedToken(\"d:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t172: AddedToken(\":8)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t173: AddedToken(\":-7\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t174: AddedToken(\"):-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t175: AddedToken(\":/\\)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t176: AddedToken(\"8(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t177: AddedToken(\"([(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t178: AddedToken(\":-(*)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t179: AddedToken(\"&-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t180: AddedToken(\":-e\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t181: AddedToken(\":(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t182: AddedToken(\":,(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t183: AddedToken(\":-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t184: AddedToken(\":-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t185: AddedToken(\":-S\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t186: AddedToken(\":-C\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t187: AddedToken(\":-r\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t188: AddedToken(\":-t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t189: AddedToken(\":-W\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t190: AddedToken(\"X-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t191: AddedToken(\"l-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t192: AddedToken(\"l:-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t193: AddedToken(\"$-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t194: AddedToken(\":-!\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t195: AddedToken(\":----}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t196: AddedToken(\"=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t197: AddedToken(\"=:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t198: AddedToken(\"3:[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t199: AddedToken(\"8<:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t200: AddedToken(\":#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t201: AddedToken(\"8-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t202: AddedToken(\"B-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t203: AddedToken(\"8-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t204: AddedToken(\"|-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t205: AddedToken(\"H-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t206: AddedToken(\"]-I\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t207: AddedToken(\"V^J\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t208: AddedToken(\"+-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t209: AddedToken(\"~:-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t210: AddedToken(\"`'\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t211: AddedToken(\"L-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t212: AddedToken(\"BI\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t213: AddedToken(\"O|\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t214: AddedToken(\"^^\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t215: AddedToken(\"ㅜㅜ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t216: AddedToken(\"ㅠㅠ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t217: AddedToken(\"ㅡㅡ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t218: AddedToken(\"😠\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t219: AddedToken(\"👿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t220: AddedToken(\"😧\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t221: AddedToken(\"😰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t222: AddedToken(\"😲\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t223: AddedToken(\"😁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t224: AddedToken(\"🐻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t225: AddedToken(\"🐱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t226: AddedToken(\"😹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t227: AddedToken(\"😼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t228: AddedToken(\"🤡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t229: AddedToken(\"🥶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t230: AddedToken(\"😖\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t231: AddedToken(\"😕\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t232: AddedToken(\"🐮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t233: AddedToken(\"🤠\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t234: AddedToken(\"😿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t235: AddedToken(\"😢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t236: AddedToken(\"😞\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t237: AddedToken(\"😵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t238: AddedToken(\"🐶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t239: AddedToken(\"😓\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t240: AddedToken(\"🐲\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t241: AddedToken(\"🤤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t242: AddedToken(\"😑\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t243: AddedToken(\"😘\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t244: AddedToken(\"😋\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t245: AddedToken(\"😱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t246: AddedToken(\"🤮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t247: AddedToken(\"🤭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t248: AddedToken(\"🤕\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t249: AddedToken(\"😷\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250: AddedToken(\"🧐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t251: AddedToken(\"😮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t252: AddedToken(\"🤨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t253: AddedToken(\"🙄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t254: AddedToken(\"😤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t255: AddedToken(\"🤬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256: AddedToken(\"😂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t257: AddedToken(\"🤒\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t258: AddedToken(\"😛\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t259: AddedToken(\"😶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t260: AddedToken(\"😨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t261: AddedToken(\"🌛\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t262: AddedToken(\"😳\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t263: AddedToken(\"🦊\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t264: AddedToken(\"🐸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t265: AddedToken(\"☹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t266: AddedToken(\"☹️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t267: AddedToken(\"😦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t268: AddedToken(\"🌝\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t269: AddedToken(\"😬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t270: AddedToken(\"😺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t271: AddedToken(\"😸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t272: AddedToken(\"😀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t273: AddedToken(\"😃\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t274: AddedToken(\"😄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t275: AddedToken(\"😅\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t276: AddedToken(\"😆\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t277: AddedToken(\"🐹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t278: AddedToken(\"🐴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t279: AddedToken(\"🥵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t280: AddedToken(\"🤗\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t281: AddedToken(\"😯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t282: AddedToken(\"😽\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t283: AddedToken(\"😗\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t284: AddedToken(\"😚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t285: AddedToken(\"😙\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t286: AddedToken(\"🌜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t287: AddedToken(\"🦁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t288: AddedToken(\"😭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t289: AddedToken(\"🤥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t290: AddedToken(\"🤦🏿‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t291: AddedToken(\"🤦🏻‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t292: AddedToken(\"🤦🏾‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t293: AddedToken(\"🤦🏼‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t294: AddedToken(\"🤦🏽‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t295: AddedToken(\"🤦‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t296: AddedToken(\"🤦🏿‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t297: AddedToken(\"🤦🏻‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t298: AddedToken(\"🤦🏾‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t299: AddedToken(\"🤦🏼‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t300: AddedToken(\"🤦🏽‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t301: AddedToken(\"🤦‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t302: AddedToken(\"🤑\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t303: AddedToken(\"🐵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t304: AddedToken(\"🐭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t305: AddedToken(\"🤢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t306: AddedToken(\"🤓\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t307: AddedToken(\"😐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t308: AddedToken(\"🌚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t309: AddedToken(\"🐼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t310: AddedToken(\"🥳\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t311: AddedToken(\"😔\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t312: AddedToken(\"😣\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t313: AddedToken(\"🤦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t314: AddedToken(\"🤦🏿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t315: AddedToken(\"🤦🏻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t316: AddedToken(\"🤦🏾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t317: AddedToken(\"🤦🏼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t318: AddedToken(\"🤦🏽\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t319: AddedToken(\"🐷\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t320: AddedToken(\"🥺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t321: AddedToken(\"😾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t322: AddedToken(\"😡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t323: AddedToken(\"🐰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t324: AddedToken(\"😌\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t325: AddedToken(\"🤖\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t326: AddedToken(\"😥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t327: AddedToken(\"🤫\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t328: AddedToken(\"😴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t329: AddedToken(\"😪\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t330: AddedToken(\"🙁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t331: AddedToken(\"🙂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t332: AddedToken(\"😻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t333: AddedToken(\"☺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t334: AddedToken(\"☺️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t335: AddedToken(\"🥰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t336: AddedToken(\"😇\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t337: AddedToken(\"😍\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t338: AddedToken(\"😈\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t339: AddedToken(\"😊\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t340: AddedToken(\"😎\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t341: AddedToken(\"😏\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t342: AddedToken(\"🤧\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t343: AddedToken(\"😝\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t344: AddedToken(\"🌞\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t345: AddedToken(\"🤔\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t346: AddedToken(\"🐯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t347: AddedToken(\"😫\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t348: AddedToken(\"😒\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t349: AddedToken(\"🦄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t350: AddedToken(\"🙃\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t351: AddedToken(\"🙀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t352: AddedToken(\"😩\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t353: AddedToken(\"🌬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t354: AddedToken(\"🌬️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t355: AddedToken(\"😉\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t356: AddedToken(\"😜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t357: AddedToken(\"🐺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t358: AddedToken(\"🤦🏿‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t359: AddedToken(\"🤦🏻‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t360: AddedToken(\"🤦🏾‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t361: AddedToken(\"🤦🏼‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t362: AddedToken(\"🤦🏽‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t363: AddedToken(\"🤦‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t364: AddedToken(\"🤦🏿‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t365: AddedToken(\"🤦🏻‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t366: AddedToken(\"🤦🏾‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t367: AddedToken(\"🤦🏼‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t368: AddedToken(\"🤦🏽‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t369: AddedToken(\"🤦‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t370: AddedToken(\"🥴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t371: AddedToken(\"😟\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t372: AddedToken(\"🥱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t373: AddedToken(\"🤪\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t374: AddedToken(\"🤐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 Prompt dictionary template, SFT dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "        \n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4 Training dataset with SFT_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT='/Users/kenny_jung/aiffel/data/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset.input_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5 Training arguments, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenny_jung/Library/Python/3.9/lib/python/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/kenny_jung/aiffel/data/KoChatGPT/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    # fp16 = True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-6 Training SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ebd5826a1b4bce9a57c08ecaf1abba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8593, 'grad_norm': 5.097161293029785, 'learning_rate': 1.6778523489932888e-05, 'epoch': 0.67}\n",
      "{'train_runtime': 1477.3997, 'train_samples_per_second': 8.122, 'train_steps_per_second': 0.508, 'train_loss': 2.799871419270833, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('/Users/kenny_jung/aiffel/data/KoChatGPT/output_1_SFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-7 Generator with Huggingface pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 불고기용 고기의 종류와 양을 알 수 없습니다. 하지만, 일반적으로 불고기용 쇠고기는 다양한 종류가 있을 수 있습니다. 예를 들어, 소고기, 돼지고기, 양파, 마늘, 고추장, 참기름 등이 있을 수 있습니다.\\n\\n일반적으로 불\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 41대 부통령직을 수행했습니다. Johnson, George B. Richardson, Robert D. Lewis, Michael F. Smith, David W. Knifford, Walter S. Benjamin, Pa\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'시카고 오 헤어 국제공항은 미국 캘리포니아주 로스앤젤레스에 위치해 있습니다. Canada Operator, California Pacific Airports, North Carolina Highway, Los Angeles Corporation, Translation of the\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 미세먼지 정보를 알 수 없습니다. 하지만 미세먼지 예보를 확인하려면 해당 지역의 환경청이나 보건당국에 문의하시는 것이 좋을 것 같습니다. 감사합니다.\\n\\n미세먼지 예보란 미세먼지 농도를 측정하고 이에\n",
      "\n",
      "### Instruction(명령어):\n",
      "봄이 오면 뭘 할까?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이므로, 계절에 대한 정보를 알 수 없습니다. 하지만, 일반적으로 봄에는 봄, 여름, 가을, 겨울 등 다양한 계절이 존재합니다. 봄에는 꽃이 만개하고, 가을에는 눈이 내립니다. 봄에는 꽃망울을 터뜨리고, 가을은 시원한 바람이 불고, 겨울에는 따뜻한 바람이 불기 때문에 봄의\n",
      "\n",
      "### Instruction(명령어):\n",
      "미래 데이터 사이언스의 전망은?\n",
      "\n",
      "### Response(응답):'미래 데이터 사이언의 전망은 매우 다양합니다.\\n\\n미래 데이터 사이언은 다양한 분야에서 사용될 수 있습니다. 예를 들어, 컴퓨터, 스마트폰, 태블릿 등 다양한 분야에서의 데이터 사이언스들이 사용됩니다.\\n또한, 미래 데이터 사이언스는 다양한 분야에서 사용되고 있습니다. 예를 들어, 인공지능, 빅데이터\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='/Users/kenny_jung/aiffel/data/KoChatGPT/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,                # beam search를 사용하는 경우, 몇 개의 beam을 사용할지 결정합니다.\n",
    "    repetition_penalty=2.0,     # 중복을 얼마나 허용할지에 대한 penalty 값입니다.\n",
    "    no_repeat_ngram_size=4,     # 중복을 허용하지 않을 n-gram 크기입니다.\n",
    "    eos_token_id=375,           # 문장의 끝을 알려주는 token의 id 값입니다.   \n",
    "    max_new_tokens=64,          # 생성할 token의 최대 개수입니다.\n",
    "    do_sample=True,             # 다양성을 위해 sampling을 사용할지 여부입니다.\n",
    "    top_k=50,                   # 다양성을 위한 top-k sampling의 k 값입니다.\n",
    "    early_stopping=True         # 생성된 문장이 끝났다고 판단할지 여부입니다.\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?',\n",
    "               '봄이 오면 뭘 할까?',\n",
    "               '미래 데이터 사이언스의 전망은?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-8 Empty cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Reward Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting colossalai==0.2.7\n",
      "  Downloading colossalai-0.2.7.tar.gz (686 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m686.7/686.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from colossalai==0.2.7) (1.26.3)\n",
      "Requirement already satisfied: tqdm in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from colossalai==0.2.7) (4.66.1)\n",
      "Requirement already satisfied: psutil in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from colossalai==0.2.7) (5.9.7)\n",
      "Requirement already satisfied: packaging in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from colossalai==0.2.7) (23.2)\n",
      "Requirement already satisfied: pre-commit in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from colossalai==0.2.7) (3.7.0)\n",
      "Requirement already satisfied: rich in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from colossalai==0.2.7) (13.7.0)\n",
      "Requirement already satisfied: click in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from colossalai==0.2.7) (8.1.7)\n",
      "Requirement already satisfied: fabric in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from colossalai==0.2.7) (3.2.2)\n",
      "Requirement already satisfied: contexttimer in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from colossalai==0.2.7) (0.3.3)\n",
      "Requirement already satisfied: ninja in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from colossalai==0.2.7) (1.11.1.1)\n",
      "Requirement already satisfied: torch in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from colossalai==0.2.7) (2.2.1)\n",
      "Requirement already satisfied: invoke>=2.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from fabric->colossalai==0.2.7) (2.2.0)\n",
      "Requirement already satisfied: paramiko>=2.4 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from fabric->colossalai==0.2.7) (3.4.0)\n",
      "Requirement already satisfied: decorator>=5 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from fabric->colossalai==0.2.7) (5.1.1)\n",
      "Requirement already satisfied: deprecated>=1.2 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from fabric->colossalai==0.2.7) (1.2.14)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from pre-commit->colossalai==0.2.7) (3.4.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from pre-commit->colossalai==0.2.7) (2.5.35)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from pre-commit->colossalai==0.2.7) (1.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from pre-commit->colossalai==0.2.7) (6.0.1)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from pre-commit->colossalai==0.2.7) (20.25.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from rich->colossalai==0.2.7) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from rich->colossalai==0.2.7) (2.17.2)\n",
      "Requirement already satisfied: filelock in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from torch->colossalai==0.2.7) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from torch->colossalai==0.2.7) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from torch->colossalai==0.2.7) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from torch->colossalai==0.2.7) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from torch->colossalai==0.2.7) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from torch->colossalai==0.2.7) (2023.12.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from deprecated>=1.2->fabric->colossalai==0.2.7) (1.14.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->colossalai==0.2.7) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from nodeenv>=0.11.1->pre-commit->colossalai==0.2.7) (58.0.4)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (4.1.2)\n",
      "Requirement already satisfied: cryptography>=3.3 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (42.0.5)\n",
      "Requirement already satisfied: pynacl>=1.5 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (1.5.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (0.3.8)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (4.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from jinja2->torch->colossalai==0.2.7) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from sympy->torch->colossalai==0.2.7) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.21)\n",
      "Building wheels for collected packages: colossalai\n",
      "  Building wheel for colossalai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for colossalai: filename=colossalai-0.2.7-py3-none-any.whl size=896497 sha256=ff2c655e9b5b2c3fcc52680474f3b24571cc6f409b7b3d09691abf5899c0d50b\n",
      "  Stored in directory: /Users/kenny_jung/Library/Caches/pip/wheels/27/f9/c8/528b9395ff61730879b490615aa4c57dce8b12b4002c225b6d\n",
      "Successfully built colossalai\n",
      "Installing collected packages: colossalai\n",
      "  Attempting uninstall: colossalai\n",
      "    Found existing installation: colossalai 0.3.6\n",
      "    Uninstalling colossalai-0.3.6:\n",
      "      Successfully uninstalled colossalai-0.3.6\n",
      "Successfully installed colossalai-0.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install colossalai==0.2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__) # 1.12.1 이상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==1.13.1\n",
      "  Downloading torch-1.13.1-cp39-none-macosx_11_0_arm64.whl.metadata (23 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from torch==1.13.1) (4.9.0)\n",
      "Downloading torch-1.13.1-cp39-none-macosx_11_0_arm64.whl (53.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0\n",
      "    Uninstalling torch-1.9.0:\n",
      "      Successfully uninstalled torch-1.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.2.1 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
      "torchvision 0.17.1 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- ---------------\n",
      "absl-py                      1.4.0\n",
      "accelerate                   0.28.0\n",
      "affine                       2.4.0\n",
      "aiohttp                      3.9.3\n",
      "aiosignal                    1.3.1\n",
      "altgraph                     0.17.2\n",
      "annotated-types              0.6.0\n",
      "anyio                        4.3.0\n",
      "appnope                      0.1.3\n",
      "array-record                 0.4.1\n",
      "asteval                      0.9.31\n",
      "asttokens                    2.4.1\n",
      "astunparse                   1.6.3\n",
      "async-timeout                4.0.3\n",
      "attrs                        23.2.0\n",
      "bcrypt                       4.1.2\n",
      "beautifulsoup4               4.12.2\n",
      "bs4                          0.0.1\n",
      "cachetools                   5.3.2\n",
      "certifi                      2023.11.17\n",
      "cffi                         1.16.0\n",
      "cfgv                         3.4.0\n",
      "chardet                      5.2.0\n",
      "charset-normalizer           3.3.2\n",
      "chatgpt                      2.2212.0\n",
      "click                        8.1.7\n",
      "click-plugins                1.1.1\n",
      "cligj                        0.7.2\n",
      "cloudpickle                  3.0.0\n",
      "colossalai                   0.2.7\n",
      "comm                         0.2.1\n",
      "contexttimer                 0.3.3\n",
      "contourpy                    1.2.0\n",
      "cramjam                      2.8.1\n",
      "cryptography                 42.0.5\n",
      "cssselect                    1.2.0\n",
      "cycler                       0.12.1\n",
      "Cython                       3.0.8\n",
      "dataclasses-json             0.6.4\n",
      "datasets                     2.18.0\n",
      "debugpy                      1.8.0\n",
      "decorator                    5.1.1\n",
      "Deprecated                   1.2.14\n",
      "dill                         0.3.8\n",
      "distlib                      0.3.8\n",
      "distro                       1.9.0\n",
      "dm-tree                      0.1.8\n",
      "einops                       0.7.0\n",
      "etils                        1.5.2\n",
      "exceptiongroup               1.2.0\n",
      "executing                    2.0.1\n",
      "fabric                       3.2.2\n",
      "faiss-cpu                    1.8.0\n",
      "fastjsonschema               2.19.1\n",
      "fastparquet                  2023.10.1\n",
      "feedfinder2                  0.0.4\n",
      "feedparser                   6.0.11\n",
      "filelock                     3.13.1\n",
      "finance-datareader           0.9.70\n",
      "flatbuffers                  23.5.26\n",
      "fonttools                    4.47.0\n",
      "frozenlist                   1.4.1\n",
      "fsspec                       2023.12.2\n",
      "future                       0.18.2\n",
      "gast                         0.5.4\n",
      "gdown                        5.1.0\n",
      "gensim                       4.3.2\n",
      "geographiclib                2.0\n",
      "geopy                        2.4.1\n",
      "google                       3.0.0\n",
      "google-auth                  2.26.2\n",
      "google-auth-oauthlib         1.2.0\n",
      "google-pasta                 0.2.0\n",
      "googleapis-common-protos     1.63.0\n",
      "graphviz                     0.20.1\n",
      "grpcio                       1.60.0\n",
      "h11                          0.14.0\n",
      "h5py                         3.10.0\n",
      "html5lib                     1.1\n",
      "httpcore                     1.0.4\n",
      "httpx                        0.27.0\n",
      "huggingface-hub              0.21.4\n",
      "identify                     2.5.35\n",
      "idna                         3.6\n",
      "imageio                      2.34.0\n",
      "imbalanced-learn             0.12.0\n",
      "importlib-metadata           7.0.1\n",
      "importlib-resources          6.1.1\n",
      "install                      1.3.5\n",
      "invoke                       2.2.0\n",
      "ipykernel                    6.28.0\n",
      "ipympl                       0.9.3\n",
      "ipython                      8.18.1\n",
      "ipython-genutils             0.2.0\n",
      "ipywidgets                   8.1.1\n",
      "jedi                         0.19.1\n",
      "jieba3k                      0.35.1\n",
      "Jinja2                       3.1.3\n",
      "joblib                       1.3.2\n",
      "JPype1                       1.5.0\n",
      "jsonpatch                    1.33\n",
      "jsonpointer                  2.4\n",
      "jsonschema                   4.20.0\n",
      "jsonschema-specifications    2023.12.1\n",
      "jupyter_client               8.6.0\n",
      "jupyter_core                 5.7.0\n",
      "jupyterlab-widgets           3.0.9\n",
      "kagglehub                    0.1.9\n",
      "keras                        2.15.0\n",
      "keras-core                   0.1.7\n",
      "keras-nlp                    0.8.0\n",
      "keras-utils                  1.0.13\n",
      "kiwisolver                   1.4.5\n",
      "konlpy                       0.6.0\n",
      "koreanize-matplotlib         0.1.1\n",
      "langchain                    0.1.11\n",
      "langchain-community          0.0.27\n",
      "langchain-core               0.1.30\n",
      "langchain-experimental       0.0.53\n",
      "langchain-openai             0.0.8\n",
      "langchain-text-splitters     0.0.1\n",
      "langchainhub                 0.1.15\n",
      "langsmith                    0.1.23\n",
      "lazy_loader                  0.3\n",
      "libclang                     16.0.6\n",
      "libs                         0.0.10\n",
      "llvmlite                     0.42.0\n",
      "lmfit                        1.2.2\n",
      "loralib                      0.1.2\n",
      "lxml                         5.1.0\n",
      "macholib                     1.15.2\n",
      "Markdown                     3.5.2\n",
      "markdown-it-py               3.0.0\n",
      "MarkupSafe                   2.1.3\n",
      "marshmallow                  3.21.1\n",
      "matplotlib                   3.8.2\n",
      "matplotlib-inline            0.1.6\n",
      "mdurl                        0.1.2\n",
      "mecab-ko-dic                 1.0.0\n",
      "mecab-python                 0.996\n",
      "mecab-python3                1.0.8\n",
      "missingno                    0.5.2\n",
      "ml-dtypes                    0.3.2\n",
      "mpmath                       1.3.0\n",
      "msgpack                      1.0.8\n",
      "multidict                    6.0.5\n",
      "multiprocess                 0.70.16\n",
      "mypy-extensions              1.0.0\n",
      "namex                        0.0.7\n",
      "nbformat                     5.9.2\n",
      "nest-asyncio                 1.5.8\n",
      "networkx                     3.2.1\n",
      "newspaper3k                  0.2.8\n",
      "ninja                        1.11.1.1\n",
      "nltk                         3.8.1\n",
      "nodeenv                      1.8.0\n",
      "numba                        0.59.0\n",
      "numpy                        1.26.3\n",
      "oauthlib                     3.2.2\n",
      "openai                       1.13.3\n",
      "opencv-python                4.9.0.80\n",
      "opt-einsum                   3.3.0\n",
      "orjson                       3.9.15\n",
      "packaging                    23.2\n",
      "pandas                       2.1.4\n",
      "pandas-datareader            0.10.0\n",
      "pandasql                     0.7.3\n",
      "paramiko                     3.4.0\n",
      "parso                        0.8.3\n",
      "patsy                        0.5.6\n",
      "pexpect                      4.9.0\n",
      "pillow                       10.2.0\n",
      "pip                          24.0\n",
      "platformdirs                 4.1.0\n",
      "plotly                       5.18.0\n",
      "pmdarima                     2.0.4\n",
      "pre-commit                   3.7.0\n",
      "promise                      2.3\n",
      "prompt-toolkit               3.0.43\n",
      "protobuf                     3.20.3\n",
      "psutil                       5.9.7\n",
      "ptyprocess                   0.7.0\n",
      "pure-eval                    0.2.2\n",
      "py-hanspell                  1.1\n",
      "pyarrow                      15.0.0\n",
      "pyarrow-hotfix               0.6\n",
      "pyasn1                       0.5.1\n",
      "pyasn1-modules               0.3.0\n",
      "pycparser                    2.21\n",
      "pydantic                     2.6.3\n",
      "pydantic_core                2.16.3\n",
      "pydot                        2.0.0\n",
      "Pygments                     2.17.2\n",
      "pykospacing                  0.5\n",
      "PyNaCl                       1.5.0\n",
      "pyparsing                    3.1.1\n",
      "PySocks                      1.7.1\n",
      "python-dateutil              2.8.2\n",
      "pytz                         2023.3.post1\n",
      "PyYAML                       6.0.1\n",
      "pyzmq                        25.1.2\n",
      "rasterio                     1.3.9\n",
      "ray                          2.10.0\n",
      "referencing                  0.32.1\n",
      "regex                        2023.12.25\n",
      "requests                     2.31.0\n",
      "requests-file                1.5.1\n",
      "requests-oauthlib            1.3.1\n",
      "rich                         13.7.0\n",
      "rpds-py                      0.17.1\n",
      "rsa                          4.9\n",
      "safetensors                  0.4.2\n",
      "scikit-image                 0.22.0\n",
      "scikit-learn                 1.3.2\n",
      "scikit-posthocs              0.8.1\n",
      "scipy                        1.11.4\n",
      "seaborn                      0.13.1\n",
      "sentencepiece                0.2.0\n",
      "setuptools                   58.0.4\n",
      "sgmllib3k                    1.0.0\n",
      "shap                         0.44.1\n",
      "six                          1.15.0\n",
      "sklearn                      0.0\n",
      "slicer                       0.0.7\n",
      "smart-open                   7.0.2\n",
      "sniffio                      1.3.1\n",
      "snuggs                       1.4.7\n",
      "soupsieve                    2.5\n",
      "soynlp                       0.0.493\n",
      "SQLAlchemy                   2.0.25\n",
      "stack-data                   0.6.3\n",
      "statsmodels                  0.14.1\n",
      "stumpy                       1.12.0\n",
      "sympy                        1.12\n",
      "ta                           0.9.0\n",
      "tabulate                     0.9.0\n",
      "tenacity                     8.2.3\n",
      "tensorboard                  2.15.2\n",
      "tensorboard-data-server      0.7.2\n",
      "tensorflow                   2.15.1\n",
      "tensorflow-datasets          4.9.3\n",
      "tensorflow-decision-forests  1.8.1\n",
      "tensorflow-estimator         2.15.0\n",
      "tensorflow-hub               0.16.1\n",
      "tensorflow-io-gcs-filesystem 0.34.0\n",
      "tensorflow-macos             2.15.0\n",
      "tensorflow-metadata          1.14.0\n",
      "tensorflow-metal             1.1.0\n",
      "termcolor                    2.4.0\n",
      "tf_keras                     2.16.0\n",
      "threadpoolctl                3.2.0\n",
      "tifffile                     2024.2.12\n",
      "tiktoken                     0.6.0\n",
      "tinysegmenter                0.3\n",
      "tldextract                   5.1.1\n",
      "tls-client                   1.0.1\n",
      "tokenizers                   0.15.2\n",
      "toml                         0.10.2\n",
      "torch                        2.2.1\n",
      "torchaudio                   2.2.1\n",
      "torchvision                  0.17.1\n",
      "tornado                      6.4\n",
      "tqdm                         4.66.1\n",
      "traitlets                    5.14.1\n",
      "transformers                 4.38.2\n",
      "tsfresh                      0.20.2\n",
      "types-requests               2.31.0.20240218\n",
      "typing_extensions            4.9.0\n",
      "typing-inspect               0.9.0\n",
      "tzdata                       2023.4\n",
      "uncertainties                3.1.7\n",
      "urllib3                      2.1.0\n",
      "virtualenv                   20.25.1\n",
      "wcwidth                      0.2.13\n",
      "webencodings                 0.5.1\n",
      "Werkzeug                     3.0.1\n",
      "wheel                        0.37.0\n",
      "widgetsnbextension           4.0.9\n",
      "wordcloud                    1.9.3\n",
      "wrapt                        1.14.1\n",
      "wurlitzer                    3.0.3\n",
      "xgboost                      2.0.3\n",
      "XlsxWriter                   3.2.0\n",
      "xxhash                       3.4.1\n",
      "yarl                         1.9.4\n",
      "zipp                         3.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenny_jung/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/kenny_jung/Library/Python/3.9/lib/python/site-packages/torch/library.py:130: UserWarning: Overriding a previously registered kernel for the same operator and the same dispatch key\n",
      "  operator: aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n",
      "    registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n",
      "  dispatch key: Meta\n",
      "  previous kernel: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1053\n",
      "       new kernel: registered at /dev/null:228 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:156.)\n",
      "  self.m.impl(name, dispatch_key, fn)\n",
      "/Users/kenny_jung/Library/Python/3.9/lib/python/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3 Model, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4 Ranking dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '차는 앞에 세워놔도 되죠?', 'chosen': '저는 AI 가 차량운전관련 미관없는 대화를 하는 것을 지양하고 있습니다. 차량 주차에 대한 정확한 정보를 입력하시면 그에 따른 답변을 드리도록 하겠습니다.', 'rejected': '가능합니다. 단, 뒤에 앉는 승객이 충분한 공간을 보장받을 수 있을 정도로 적절히 떨어져야 합니다.'}\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/kenny_jung/aiffel/data/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5 Shuffle and ranking dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은?', 'chosen': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은 류승완의 사무실입니다.', 'rejected': '대구 영화사옥'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2556.77it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 2695.46it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "흑고래의 무게는 어느 정도야\n",
      "######################################################################\n",
      "## chosen ##\n",
      "흑고래의 평균 몸무게는 약 25~40톤 정도이지만, 최대 몸무게는 50톤 이상에 이를 수 있습니다.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "흑고래의 무게는 매우 다양하게 달라집니다. 약 200kg에서 10톤까지 달라질 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-6 RM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1,\n",
    "                             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]loc(\"mps_reshape_2\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/4e1473ee-9f66-11ee-8daf-cedaeb4cabe2/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":303:0)): error: 'anec.reshape' op failed: input tensor dimensions are not supported on ANEs.\n",
      "loc(\"Cast back to bool\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/4e1473ee-9f66-11ee-8daf-cedaeb4cabe2/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x1x512x512xi1>'\n",
      "Train epoch:   0%|          | 0/1 [04:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_lora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/kenny_jung/aiffel/data/KoChatGPT/output_2_RM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/chatgpt/trainer/rm.py:67\u001b[0m, in \u001b[0;36mRewardModelTrainer.fit\u001b[0;34m(self, use_lora)\u001b[0m\n\u001b[1;32m     64\u001b[0m reject_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(reject_ids, attention_mask\u001b[38;5;241m=\u001b[39mr_mask)\n\u001b[1;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(chosen_reward, reject_reward)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/chatgpt/trainer/strategies/naive.py:19\u001b[0m, in \u001b[0;36mNaiveStrategy.backward\u001b[0;34m(self, loss, model, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss: torch\u001b[38;5;241m.\u001b[39mTensor, model: nn\u001b[38;5;241m.\u001b[39mModule, optimizer: optim\u001b[38;5;241m.\u001b[39mOptimizer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.fit(use_lora=0)\n",
    "model.save_pretrained('/Users/kenny_jung/aiffel/data/KoChatGPT/output_2_RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_lora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/kenny_jung/aiffel/data/KoChatGPT/output_2_RM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/chatgpt/trainer/rm.py:61\u001b[0m, in \u001b[0;36mRewardModelTrainer.fit\u001b[0;34m(self, use_lora)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chosen_ids, c_mask, reject_ids, r_mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader:\n\u001b[0;32m---> 61\u001b[0m     chosen_ids \u001b[38;5;241m=\u001b[39m \u001b[43mchosen_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     c_mask \u001b[38;5;241m=\u001b[39m c_mask\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     63\u001b[0m     reject_ids \u001b[38;5;241m=\u001b[39m reject_ids\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/cuda/__init__.py:221\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
