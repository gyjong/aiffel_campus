{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenny_jung/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_mrpc_dataset = load_dataset('glue', 'mrpc')\n",
    "print(huggingface_mrpc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence1', 'sentence2', 'label', 'idx']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = huggingface_mrpc_dataset['test']\n",
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_subset = train.select(range(10)).shuffle()\n",
    "print(train_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 : PCCW 's chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So .\n",
      "sentence2 : Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So .\n",
      "label : 1\n",
      "idx : 0\n",
      "\n",
      "\n",
      "sentence1 : The world 's two largest automakers said their U.S. sales declined more than predicted last month as a late summer sales frenzy caused more of an industry backlash than expected .\n",
      "sentence2 : Domestic sales at both GM and No. 2 Ford Motor Co. declined more than predicted as a late summer sales frenzy prompted a larger-than-expected industry backlash .\n",
      "label : 1\n",
      "idx : 1\n",
      "\n",
      "\n",
      "sentence1 : According to the federal Centers for Disease Control and Prevention ( news - web sites ) , there were 19 reported cases of measles in the United States in 2002 .\n",
      "sentence2 : The Centers for Disease Control and Prevention said there were 19 reported cases of measles in the United States in 2002 .\n",
      "label : 1\n",
      "idx : 2\n",
      "\n",
      "\n",
      "sentence1 : A tropical storm rapidly developed in the Gulf of Mexico Sunday and was expected to hit somewhere along the Texas or Louisiana coasts by Monday night .\n",
      "sentence2 : A tropical storm rapidly developed in the Gulf of Mexico on Sunday and could have hurricane-force winds when it hits land somewhere along the Louisiana coast Monday night .\n",
      "label : 0\n",
      "idx : 3\n",
      "\n",
      "\n",
      "sentence1 : The company didn 't detail the costs of the replacement and repairs .\n",
      "sentence2 : But company officials expect the costs of the replacement work to run into the millions of dollars .\n",
      "label : 0\n",
      "idx : 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset from tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 12:02:49.332292: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2024-03-26 12:02:49.332315: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2024-03-26 12:02:49.332318: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2024-03-26 12:02:49.332341: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-26 12:02:49.332362: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from datasets import Dataset\n",
    "\n",
    "tf_dataset, tf_dataset_info = tfds.load('glue/mrpc', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 : tf.Tensor(b'The identical rovers will act as robotic geologists , searching for evidence of past water .', shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b'The rovers act as robotic geologists , moving on six wheels .', shape=(), dtype=string)\n",
      "label : tf.Tensor(0, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(1680, shape=(), dtype=int32)\n",
      "\n",
      "\n",
      "sentence1 : tf.Tensor(b\"Less than 20 percent of Boise 's sales would come from making lumber and paper after the OfficeMax purchase is completed .\", shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b\"Less than 20 percent of Boise 's sales would come from making lumber and paper after the OfficeMax purchase is complete , assuming those businesses aren 't sold .\", shape=(), dtype=string)\n",
      "label : tf.Tensor(0, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(1456, shape=(), dtype=int32)\n",
      "\n",
      "\n",
      "sentence1 : tf.Tensor(b'Spider-Man snatched $ 114.7 million in its debut last year and went on to capture $ 403.7 million .', shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b'Spider-Man , rated PG-13 , snatched $ 114.7 million in its first weekend and went on to take in $ 403.7 million .', shape=(), dtype=string)\n",
      "label : tf.Tensor(1, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(3017, shape=(), dtype=int32)\n",
      "\n",
      "\n",
      "sentence1 : tf.Tensor(b\"The 2002 second quarter results don 't include figures from our friends at Compaq .\", shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b'The year-ago numbers do not include figures from Compaq Computer .', shape=(), dtype=string)\n",
      "label : tf.Tensor(1, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(2896, shape=(), dtype=int32)\n",
      "\n",
      "\n",
      "sentence1 : tf.Tensor(b'Solomon 5.5 is available initially in the United States and Canada , for a starting price of about $ 12,700 .', shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b'Solomon 5.5 is now available in the U.S. and Canada through Microsoft Business Solutions resellers .', shape=(), dtype=string)\n",
      "label : tf.Tensor(0, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(499, shape=(), dtype=int32)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 12:03:00.621897: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "examples = tf_dataset['train'].take(5)\n",
    "\n",
    "for example in examples:\n",
    "    for col in cols:\n",
    "        print(col, \":\", example[col])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow dataset 구조를 python dict 형식으로 변경\n",
    "# Dataset이 train, validation, test로 나뉘도록 구성\n",
    "train_dataset = tfds.as_dataframe(tf_dataset['train'], tf_dataset_info)\n",
    "val_dataset = tfds.as_dataframe(tf_dataset['validation'], tf_dataset_info)\n",
    "test_dataset = tfds.as_dataframe(tf_dataset['test'], tf_dataset_info)\n",
    "\n",
    "# dataframe 데이터를 dict 내부에 list로 변경\n",
    "train_dataset = train_dataset.to_dict('list')\n",
    "val_dataset = val_dataset.to_dict('list')\n",
    "test_dataset = test_dataset.to_dict('list')\n",
    "\n",
    "# Huggingface dataset\n",
    "tf_train_dataset = Dataset.from_dict(train_dataset)\n",
    "tf_val_dataset = Dataset.from_dict(val_dataset)\n",
    "tf_test_dataset = Dataset.from_dict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163</td>\n",
       "      <td>-1</td>\n",
       "      <td>b'Shares in BA were down 1.5 percent at 168 pe...</td>\n",
       "      <td>b'Shares in BA were down three percent at 165-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>131</td>\n",
       "      <td>-1</td>\n",
       "      <td>b'The South Korean Agriculture and Forestry Mi...</td>\n",
       "      <td>b'The South Korean Agriculture and Forestry Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1579</td>\n",
       "      <td>-1</td>\n",
       "      <td>b'\" New Yorkers didn \\'t embrace these units l...</td>\n",
       "      <td>b'\" New Yorkers didn \\'t embrace these units l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1151</td>\n",
       "      <td>-1</td>\n",
       "      <td>b'\" I really liked him and I still do , \" Cohe...</td>\n",
       "      <td>b'And I really liked him , and I still do .'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021</td>\n",
       "      <td>-1</td>\n",
       "      <td>b'Tight media controls and the remote location...</td>\n",
       "      <td>b'Tight media controls and the remote location...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx  label                                          sentence1  \\\n",
       "0   163     -1  b'Shares in BA were down 1.5 percent at 168 pe...   \n",
       "1   131     -1  b'The South Korean Agriculture and Forestry Mi...   \n",
       "2  1579     -1  b'\" New Yorkers didn \\'t embrace these units l...   \n",
       "3  1151     -1  b'\" I really liked him and I still do , \" Cohe...   \n",
       "4  1021     -1  b'Tight media controls and the remote location...   \n",
       "\n",
       "                                           sentence2  \n",
       "0  b'Shares in BA were down three percent at 165-...  \n",
       "1  b'The South Korean Agriculture and Forestry Mi...  \n",
       "2  b'\" New Yorkers didn \\'t embrace these units l...  \n",
       "3       b'And I really liked him , and I still do .'  \n",
       "4  b'Tight media controls and the remote location...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(test_dataset)\n",
    "train_df.columns\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5535fd0abb5468a8a468761bcc2184a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f61196082d941a685feffb25f25c580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7989db4447664a09a8056e1e9e51482d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_tf(batch):\n",
    "    sentence1 = [s.decode('utf-8') for s in batch['sentence1']]\n",
    "    sentence2 = [s.decode('utf-8') for s in batch['sentence2']]\n",
    "    return huggingface_tokenizer(\n",
    "        sentence1,\n",
    "        sentence2,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "# 토큰화 및 패딩을 적용\n",
    "tf_train_dataset = tf_train_dataset.map(transform_tf, batched=True)\n",
    "tf_val_dataset = tf_val_dataset.map(transform_tf, batched=True)\n",
    "tf_test_dataset = tf_test_dataset.map(transform_tf, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return huggingface_tokenizer(\n",
    "        data['sentence1'],\n",
    "        data['sentence2'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = huggingface_mrpc_dataset.map(transform, batched=True)\n",
    "\n",
    "# train & validation & test split\n",
    "hf_train_dataset = hf_dataset['train']\n",
    "hf_val_dataset = hf_dataset['validation']\n",
    "hf_test_dataset = hf_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 408\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1725\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Evaluation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from accelerate) (5.9.7)\n",
      "Requirement already satisfied: pyyaml in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from accelerate) (2.2.1)\n",
      "Requirement already satisfied: huggingface-hub in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from accelerate) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\n",
      "Requirement already satisfied: requests in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub->accelerate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kenny_jung/Library/Python/3.9/lib/python/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install transformers[torch]\n",
    "# %pip install accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Trainer를 사용하기 위해서는 TrainingArguments를 통해 학습 관련 설정을 미리 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = '/Users/kenny_jung/aiffel/data/hf_transformer'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                            # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           # evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                  # learning_rate\n",
    "    per_device_train_batch_size = 8,       # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,        # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                  # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                   # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. task가 classification인지 regression인지에 따라 모델의 출력 형태가 달라지므로 task별로 적합한 출력 형식을 고려해 모델의 성능을 계산하는 방법을 미리 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenny_jung/Library/Python/3.9/lib/python/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('glue', 'mrpc')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Trainer에 model, arguments, train_dataset, eval_dataset, compute_metrics를 넣고 train을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenny_jung/Library/Python/3.9/lib/python/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6aba435894743e2a69c80d83c393550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdb482f48d440c0946aa7ecd71cffda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3841363489627838, 'eval_accuracy': 0.8382352941176471, 'eval_f1': 0.8838028169014085, 'eval_runtime': 5.9767, 'eval_samples_per_second': 68.265, 'eval_steps_per_second': 8.533, 'epoch': 1.0}\n",
      "{'loss': 0.5152, 'grad_norm': 21.535314559936523, 'learning_rate': 1.2737835875090777e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc95a3d663243eabecbd121d6eac57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3564828634262085, 'eval_accuracy': 0.8431372549019608, 'eval_f1': 0.8865248226950355, 'eval_runtime': 5.0177, 'eval_samples_per_second': 81.312, 'eval_steps_per_second': 10.164, 'epoch': 2.0}\n",
      "{'loss': 0.3235, 'grad_norm': 0.5208008885383606, 'learning_rate': 5.475671750181555e-06, 'epoch': 2.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f17bc6151d4071b4e8f2908b760973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.516402006149292, 'eval_accuracy': 0.8406862745098039, 'eval_f1': 0.8881239242685025, 'eval_runtime': 5.0712, 'eval_samples_per_second': 80.455, 'eval_steps_per_second': 10.057, 'epoch': 3.0}\n",
      "{'train_runtime': 533.5478, 'train_samples_per_second': 20.624, 'train_steps_per_second': 2.581, 'train_loss': 0.3688121177878342, 'epoch': 3.0}\n",
      "슝~\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_train_dataset,    # training dataset\n",
    "    eval_dataset=hf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Test data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49edffd970fc418fa9cc9ebd606f60d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/216 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5602657794952393,\n",
       " 'eval_accuracy': 0.831304347826087,\n",
       " 'eval_f1': 0.8758002560819462,\n",
       " 'eval_runtime': 20.9762,\n",
       " 'eval_samples_per_second': 82.236,\n",
       " 'eval_steps_per_second': 10.297,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 메모리 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#메모리를 비워줍니다.\n",
    "del huggingface_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 커스텀 데이터셋으로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenny_jung/Library/Python/3.9/lib/python/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5d2673b4ba433b909cbdd1253970ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c122189cd4854e0ea93dcd04c431b951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39602720737457275, 'eval_accuracy': 0.8308823529411765, 'eval_f1': 0.8685714285714285, 'eval_runtime': 5.1938, 'eval_samples_per_second': 78.556, 'eval_steps_per_second': 9.819, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory /Users/kenny_jung/aiffel/data/hf_transformer/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5188, 'grad_norm': 10.028202056884766, 'learning_rate': 1.2737835875090777e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5797d695f97644f39316356e76c5fc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34312117099761963, 'eval_accuracy': 0.8382352941176471, 'eval_f1': 0.8825622775800711, 'eval_runtime': 5.2577, 'eval_samples_per_second': 77.6, 'eval_steps_per_second': 9.7, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory /Users/kenny_jung/aiffel/data/hf_transformer/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3262, 'grad_norm': 1.7467845678329468, 'learning_rate': 5.475671750181555e-06, 'epoch': 2.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e1aeaa585745c3b8d2597ad4511929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5077135562896729, 'eval_accuracy': 0.8357843137254902, 'eval_f1': 0.8842832469775476, 'eval_runtime': 5.2442, 'eval_samples_per_second': 77.801, 'eval_steps_per_second': 9.725, 'epoch': 3.0}\n",
      "{'train_runtime': 523.16, 'train_samples_per_second': 21.034, 'train_steps_per_second': 2.632, 'train_loss': 0.36601108836365853, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.36601108836365853, metrics={'train_runtime': 523.16, 'train_samples_per_second': 21.034, 'train_steps_per_second': 2.632, 'train_loss': 0.36601108836365853, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=tf_train_dataset,    # training dataset\n",
    "    eval_dataset=tf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9728c9e580b64ebfb6dab6584dad55e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/216 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5508893132209778,\n",
       " 'eval_accuracy': 0.8226086956521739,\n",
       " 'eval_f1': 0.8685567010309279,\n",
       " 'eval_runtime': 20.8552,\n",
       " 'eval_samples_per_second': 82.713,\n",
       " 'eval_steps_per_second': 10.357,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_built())\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "mps_device = torch.device(\"mps\")\n",
    "\n",
    "# MPS 장치에 바로 tensor를 생성합니다.\n",
    "x = torch.ones(5, device=mps_device)\n",
    "# 또는\n",
    "x = torch.ones(5, device=\"mps\")\n",
    "\n",
    "# GPU 상에서 연산을 진행합니다.\n",
    "y = x * 2\n",
    "\n",
    "# 또는, 다른 장치와 마찬가지로 MPS로 이동할 수도 있습니다.\n",
    "model = YourFavoriteNet()  # 어떤 모델의 객체를 생성한 뒤,\n",
    "model.to(mps_device)       # MPS 장치로 이동합니다.\n",
    "\n",
    "# 이제 모델과 텐서를 호출하면 GPU에서 연산이 이뤄집니다.\n",
    "pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:2.2.1\n",
      "MPS 장치를 지원하도록 build 되었는지: True\n",
      "MPS 장치가 사용 가능한지: True\n",
      "zsh:1: command not found: python\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version:{torch.__version__}\") # 1.12.1 이상\n",
    "print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\") # True 여야 합니다.\n",
    "print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\") # True 여야 합니다.\n",
    "!python -c 'import platform;print(platform.platform())'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
